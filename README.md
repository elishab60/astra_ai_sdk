# ASTRA AI SDK

![AperÃ§u de l'interface](./public/illustration.png)

ASTRA AI SDK est une console web Next.js pensÃ©e pour piloter une instance Ollama locale avec une expÃ©rience riche cÃ´tÃ© front et un minimum d'API cÃ´tÃ© serveur. L'application combine un chat temps rÃ©el, la gestion complÃ¨te des modÃ¨les, le monitoring de la machine hÃ´te et un bac Ã  sable d'exÃ©cution de snippets pour offrir une alternative haut de gamme Ã  l'interface Ollama standard.

## FonctionnalitÃ©s principales

- **Chat en streaming** avec rendu Markdown, coloration syntaxique et dÃ©coupe fine des blocs de code pour Ã©viter les scintillements pendant le flux token par token. ã€F:src/components/chat/chat-card.tsxâ€ L1-L120ã€‘
- **SÃ©lecteur et installation de modÃ¨les** directement depuis l'interface avec suivi prÃ©cis du tÃ©lÃ©chargement (pourcentage, dÃ©bit, ETA) et accÃ¨s rapide aux modÃ¨les dÃ©jÃ  chargÃ©s en RAM. ã€F:src/components/ModelCombobox.tsxâ€ L1-L204ã€‘
- **Barre de mÃ©triques en direct** indiquant modÃ¨le actif, RAM disponible, tokens entrants/sortants et temps de rÃ©ponse une fois la gÃ©nÃ©ration terminÃ©e. ã€F:src/components/MetricsBar.tsxâ€ L1-L63ã€‘
- **RÃ©sumÃ© systÃ¨me dÃ©taillÃ©** (CPU, GPU, RAM, disque, rÃ©seau, modÃ¨les Ollama installÃ©s/chargÃ©s) accessible via un popover, alimentÃ© par l'API `/api/system/summary`. ã€F:src/components/system-popover.tsxâ€ L1-L200ã€‘ã€F:src/app/api/system/summary/route.tsâ€ L1-L104ã€‘
- **Console avancÃ©e** (`/console`) combinant gestionnaire de sessions, catalogue Ollama, suivi des modÃ¨les en cours d'exÃ©cution et panneau d'inspection. ã€F:src/components/console/AIConsole.tsxâ€ L1-L13ã€‘ã€F:src/components/console/model-manager/ModelManager.tsxâ€ L1-L205ã€‘
- **ExÃ©cution sÃ©curisÃ©e de snippets** gÃ©nÃ©rÃ©s par l'IA (bash, Python, Node/TypeScript) grÃ¢ce Ã  un endpoint sandboxÃ© avec timeout. ã€F:src/components/chat/CodeBubble.tsxâ€ L1-L64ã€‘ã€F:src/app/api/exec/route.tsâ€ L1-L96ã€‘

## Architecture

### CÃ´tÃ© client
- Application **Next.js App Router** en mode client pour les vues interactives (`src/app/page.tsx`, `src/app/console/page.tsx`). ã€F:src/app/page.tsxâ€ L1-L188ã€‘ã€F:src/app/console/page.tsxâ€ L1-L7ã€‘
- Gestion d'Ã©tat lÃ©gÃ¨re via **Zustand** pour les sessions de chat persistantes dans la console. ã€F:src/store/session.tsâ€ L1-L33ã€‘
- Hooks dÃ©diÃ©s pour interroger pÃ©riodiquement Ollama et afficher l'Ã©tat mÃ©moire. ã€F:src/hooks/use-ollama-status.tsâ€ L1-L26ã€‘
- Composants UI basÃ©s sur Radix + shadcn/ui personnalisÃ©s pour garantir cohÃ©rence visuelle et support du thÃ¨me clair/sombre.

### CÃ´tÃ© serveur
- Routes Next.js (`app/api/*`) agissant comme **faÃ§ade locale** vers Ollama : listage, installation/suppression, streaming NDJSON des chats et collecte des mÃ©triques systÃ¨me. ã€F:src/app/api/ollama/chat/route.tsâ€ L1-L52ã€‘ã€F:src/lib/ollama.tsâ€ L1-L53ã€‘
- IntÃ©gration de la librairie [`systeminformation`](https://www.npmjs.com/package/systeminformation) pour exposer les ressources machine.
- Sandbox d'exÃ©cution (`/api/exec`) isolÃ© dans un rÃ©pertoire temporaire avec arrÃªt forcÃ© Ã  15â€¯s pour limiter les risques. ã€F:src/app/api/exec/route.tsâ€ L25-L73ã€‘

## PrÃ©requis

- **Node.js 20+** (Next.jsÂ 16 et ReactÂ 19 sont utilisÃ©s).
- **Ollama 0.4+** accessible en local ou sur le rÃ©seau. L'URL est configurable via la variable `OLLAMA_HOST` (par dÃ©faut `http://127.0.0.1:11434`). ã€F:src/lib/ollama.tsâ€ L1-L53ã€‘
- PNPM, npm ou bun pour gÃ©rer les dÃ©pendances (le projet inclut un `package-lock.json`).

## Installer Ollama via la ligne de commande

L'application suppose qu'Ollama est installÃ© **avant** de lancer le serveur Next.js. Les commandes ci-dessous permettent de dÃ©ployer une instance locale prÃªte Ã  l'emploi.

### macOS (AppleÂ Silicon)

```bash
# 1. TÃ©lÃ©charger et installer la CLI Ollama
curl -fsSL https://ollama.com/download/Ollama-darwin-arm64.tgz | sudo tar -xz -C /usr/local/bin

# 2. Lancer le service (Ã  exÃ©cuter une fois aprÃ¨s installation)
ollama serve

# 3. VÃ©rifier qu'un modÃ¨le peut Ãªtre tirÃ©
ollama pull llama3.1
```

> â„¹ï¸ Le service dÃ©marre automatiquement via launchd aprÃ¨s la premiÃ¨re exÃ©cution de `ollama serve`. Sur macOS, aucune Ã©tape supplÃ©mentaire n'est nÃ©cessaire pour exposer l'API sur `127.0.0.1:11434`.

### Linux x86_64

```bash
curl -fsSL https://ollama.com/install.sh | sh
sudo systemctl enable --now ollama
ollama pull llama3.1
```

### Windows 11/10 (WSLÂ 2 recommandÃ©)

```powershell
# Dans PowerShell administrateur
winget install Ollama.Ollama

# AprÃ¨s l'installation, ouvrir une invite PowerShell et dÃ©marrer le service
ollama serve

# Facultatif : dans WSL, exporter l'API vers le rÃ©seau local
setx OLLAMA_HOST http://127.0.0.1:11434
```

> âœ… Vous pouvez Ã©galement utiliser WSLÂ 2â€¯: installez Ollama cÃ´tÃ© distribution Linux puis exposez `OLLAMA_HOST` depuis Windows pour que Next.js puisse s'y connecter.

Une fois Ollama installÃ©, vÃ©rifiez que l'API rÃ©pondÂ :

```bash
curl http://127.0.0.1:11434/api/version
```

## DÃ©marrage rapide du projet

1. Cloner ce dÃ©pÃ´t puis installer les dÃ©pendancesÂ :
   ```bash
   git clone https://github.com/votre-org/astra-ai-sdk.git
   cd astra-ai-sdk
   npm install
   # ou
   pnpm install
   ```
2. (Optionnel) crÃ©er un fichier `.env.local` pour surcharger `OLLAMA_HOST` si l'instance n'est pas locale.
3. Lancer le serveur de dÃ©veloppement Next.jsÂ :
   ```bash
   npm run dev
   ```
4. AccÃ©der Ã  [http://localhost:3000](http://localhost:3000) pour l'interface principale ou [http://localhost:3000/console](http://localhost:3000/console) pour la console avancÃ©e.

### Build production

```bash
npm run build
npm start
```

Le build statique est prÃªt Ã  Ãªtre dÃ©ployÃ© sur n'importe quelle plateforme compatible Next.js (Vercel, Docker, VM personnelleâ€¦). Pensez Ã  exposer `OLLAMA_HOST` dans l'environnement d'exÃ©cution.

## Dimensionnement matÃ©riel recommandÃ©

Les besoins varient selon la taille du modÃ¨le Ollama chargÃ©. Le tableau suivant rÃ©capitule les configurations **minimales** et **recommandÃ©es** testÃ©es pour une expÃ©rience fluide avec les quantifications par dÃ©faut (`Q4_K_M`).

| ModÃ¨le | AppleÂ Silicon â€“ Configuration minimale | AppleÂ Silicon â€“ Configuration recommandÃ©e | Windows/WSL â€“ Configuration minimale | Windows/WSL â€“ Configuration recommandÃ©e |
| --- | --- | --- | --- | --- |
| 7â€“8B (ex. `llama3.1:8b`) | MacBook Air M1/M2, 8â€¯cÅ“urs CPU / 8â€¯Go RAM unifiÃ©e | MacBook Pro M3, 10â€¯cÅ“urs CPU / 16â€¯Go RAM, SSD NVMe | CPU 6â€¯cÅ“urs, 16â€¯Go RAM systÃ¨me, GPU 8â€¯Go VRAM (RTXÂ 3060) | CPU 8â€¯cÅ“urs, 32â€¯Go RAM, GPU 12â€¯Go VRAM (RTXÂ 4070) |
| 13B (ex. `llama3.1:13b`) | MacBook Pro M2 Pro, 10â€¯cÅ“urs / 16â€¯Go RAM | MacBook Pro M3 Pro, 12â€¯cÅ“urs / 24â€¯Go RAM | CPU 8â€¯cÅ“urs, 32â€¯Go RAM, GPU 12â€¯Go VRAM | CPU 12â€¯cÅ“urs, 48â€¯Go RAM, GPU 16â€¯Go VRAM |
| 33B (ex. `mixtral:8x7b`) | Mac Studio M2 Max, 12â€¯cÅ“urs / 32â€¯Go RAM | Mac Studio M2 Ultra, 16â€¯cÅ“urs / 48â€¯Go RAM | CPU 12â€¯cÅ“urs, 64â€¯Go RAM, GPU 20â€¯Go VRAM (RTXÂ 4080) | CPU 16â€¯cÅ“urs, 96â€¯Go RAM, GPU 24â€¯Go VRAM (RTXÂ 4090) |
| 70B (ex. `llama3.1:70b`) | Mac Studio M2 Ultra, 24â€¯cÅ“urs / 96â€¯Go RAM | Mac Pro M2 Ultra, 24â€¯cÅ“urs / 128â€¯Go RAM | CPU 16â€¯cÅ“urs, 128â€¯Go RAM, GPU 48â€¯Go VRAM (dual RTXÂ 6000 Ada) | CPU 24â€¯cÅ“urs, 192â€¯Go RAM, GPU 80â€¯Go VRAM (RTXÂ 6000 Ada x2) |

> ğŸ’¡ Avec des quantifications plus lÃ©gÃ¨res (`Q8`), augmentez la mÃ©moire d'au moins 30â€¯%. En dessous des spÃ©cifications minimales, Ollama tombera en swap et les temps de rÃ©ponse se dÃ©graderont fortement.

## Structure du projet

```
src/
â”œâ”€ app/                  # Pages Next.js (landing, console, API routes)
â”œâ”€ components/           # UI (chat, metrics, popovers, console)
â”œâ”€ hooks/                # Hooks custom (statut Ollamaâ€¦)
â”œâ”€ lib/                  # Clients Ollama, helpers NDJSON, formatage
â””â”€ store/                # Zustand store pour les sessions
```

## IntÃ©gration Ollama

- Tous les appels rÃ©seau passent par les routes Next internes qui rejouent les endpoints officiels (`/api/chat`, `/api/tags`, `/api/pull`, etc.) en ajoutant du **contrÃ´le de flux** et de la **tÃ©lÃ©mÃ©trie**. ã€F:src/app/api/ollama/pull/route.tsâ€ L1-L63ã€‘ã€F:src/app/api/ollama/status/route.tsâ€ L1-L74ã€‘
- Le chat repose sur un flux **NDJSON stream** traitÃ© cÃ´tÃ© client pour mettre Ã  jour progressivement l'UI et compter les tokens. ã€F:src/lib/ndjson.tsâ€ L1-L96ã€‘ã€F:src/components/chat/chat-card.tsxâ€ L121-L220ã€‘
- La barre de mÃ©triques et le popover systÃ¨me combinent les donnÃ©es Ollama (modÃ¨les chargÃ©s) et les ressources locales pour faciliter le **dimensionnement** et le **monitoring**. ã€F:src/components/MetricsBar.tsxâ€ L1-L63ã€‘ã€F:src/components/system-popover.tsxâ€ L135-L217ã€‘

## Console dÃ©veloppeur

La vue `/console` se destine aux power usersÂ :
- **Gestionnaire de modÃ¨les** avec recherche unifiÃ©e catalogue/installÃ©s, suivi du tÃ©lÃ©chargement et actions (installer, mettre Ã  jour, supprimer). ã€F:src/components/console/model-manager/ModelManager.tsxâ€ L1-L205ã€‘
- **Sidebar multi-sessions** pour basculer de modÃ¨le en un clic et amorcer de nouvelles conversations. ã€F:src/components/console/Sidebar.tsxâ€ L1-L47ã€‘
- **Inspector** (panneau droit) prÃªt Ã  accueillir logs et mÃ©tadonnÃ©es pendant les runs (structure dÃ©jÃ  en place dans `src/components/console/Inspector.tsx`).

## ExÃ©cution de code

Les blocs de code dÃ©tectÃ©s dans les rÃ©ponses peuvent Ãªtre exÃ©cutÃ©s si leur langage fait partie de la whitelist (`bash`, `python`, `node`, `typescript`). Les snippets sont envoyÃ©s Ã  `/api/exec` qui crÃ©e un dossier temporaire, Ã©crit le fichier, lance le processus puis stream stdout/stderr avec un timeout strict de 15Â secondes. ã€F:src/components/chat/CodeBubble.tsxâ€ L1-L64ã€‘ã€F:src/app/api/exec/route.tsâ€ L25-L96ã€‘

## Scripts utiles

- `npm run dev`Â : serveur de dÃ©veloppement avec HMR.
- `npm run lint`Â : linting ESLint (configuration Next.js 16).
- `npm run build` / `npm start`Â : build et lancement production.

## Limitations & axes d'amÃ©lioration

- Authentification et gestion multi-utilisateurs absentes (l'application suppose un usage local et maÃ®trisÃ©).
- Pas de persistance long terme des conversations (le store Zustand vit en mÃ©moire cÃ´tÃ© client).
- L'exÃ©cution de code reste volontairement limitÃ©e Ã  un set restreint de langages et Ã  15Â s d'exÃ©cution pour minimiser les risques.
- La console `/console` expose dÃ©jÃ  la structure UI mais certains panneaux (Inspector, logs stream) peuvent Ãªtre enrichis en fonction des besoins.

## Licence

Le projet est diffusÃ© sans licence explicite. Ajoutez un fichier `LICENSE` si vous souhaitez en prÃ©ciser les termes.
